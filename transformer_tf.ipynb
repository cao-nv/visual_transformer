{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_tf",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmVDl6ju9YjePMBBClLZY4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cao-nv/visual_transformer/blob/main/transformer_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azlUlEukrmKe",
        "outputId": "d2ea1d87-2395-4a4f-de04-08444fa53c2e"
      },
      "source": [
        "!pip install einops\n",
        "!pip install wandb "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.30)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy1fKDQ_2jOt",
        "outputId": "afb0862b-de60-4bd5-f125-b439f5972597"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sQbUaTnsSFO"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpRPCoCB7BPA"
      },
      "source": [
        "from einops import rearrange"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P76gQzvyvJZv"
      },
      "source": [
        "import pdb"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qY1ZiYKytyS"
      },
      "source": [
        "class Residual(layers.Layer):\n",
        "  def __init__(self, fn):\n",
        "    super().__init__()\n",
        "    self.fn = fn\n",
        "  \n",
        "  def call(self, x, **kwargs):\n",
        "    return self.fn(x, **kwargs) + x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEhP4Qwcz2nF"
      },
      "source": [
        "class PreNorm(layers.Layer):\n",
        "  def __init__(self, dim, fn):\n",
        "    super().__init__()\n",
        "    self.fn = fn\n",
        "    self.norm = layers.LayerNormalization(axis=-1)\n",
        "\n",
        "  def call(self, x, **kwargs):\n",
        "    return self.fn(self.norm(x), **kwargs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbcRl6w_0pIv"
      },
      "source": [
        "class FeedForward(layers.Layer):\n",
        "  def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "    super().__init__()\n",
        "    self.dense1 = layers.Dense(hidden_dim, input_shape=(None, dim), use_bias=False)\n",
        "    self.gelu = tfa.layers.GELU()\n",
        "    self.dropout1 = layers.Dropout(dropout)\n",
        "    self.dense2 = layers.Dense(dim, input_shape=(None, hidden_dim))\n",
        "    self.dropout2 = layers.Dropout(dropout)\n",
        "\n",
        "    # ff_layers = [layers.Dense(hidden_dim, input_shape=(None, dim), use_bias=False), \n",
        "    #           tfa.layers.GELU(), \n",
        "    #           layers.Dropout(dropout), \n",
        "    #           layers.Dense(dim), \n",
        "    #           layers.Dropout(dropout)]\n",
        "    # self.net = tf.keras.Sequential(layers=ff_layers, name=\"FeedForward\")\n",
        "\n",
        "  def call(self, x, training=True): \n",
        "    x = self.dense1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.dropout1(x, training=training)\n",
        "    x = self.dense2(x)\n",
        "    x = self.dropout2(x, training=training)\n",
        "    return x"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBkp9irk3aXC"
      },
      "source": [
        "class Attention(layers.Layer): \n",
        "  def __init__(self, dim, heads=8, dropout=0.):\n",
        "    super().__init__()\n",
        "    self.heads = heads \n",
        "    self.scales = heads ** (-0.5)\n",
        "\n",
        "    self.to_qkv = layers.Dense(dim*3, input_shape=(dim,), use_bias=False)\n",
        "\n",
        "    self.out_dense = layers.Dense(dim, input_shape=(None, dim))\n",
        "    self.out_dropout = layers.Dropout(dropout)\n",
        "\n",
        "    self.to_out = tf.keras.Sequential(layers=[layers.Dense(dim, input_shape=(None, dim)), \n",
        "                                              layers.Dropout(dropout)], name='to_out')\n",
        "    \n",
        "  def to_out(self, x, training=True):\n",
        "    out = self.out_dense(x)\n",
        "    out = self.out_dropout(out, training=training)\n",
        "    return out\n",
        "\n",
        "  def call(self, x, mask=None, training=True):\n",
        "    b, n, _, h = *x.shape, self.heads\n",
        "    qkv = tf.split(self.to_qkv(x), 3, axis=-1)\n",
        "    q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), qkv)\n",
        "\n",
        "    dots = tf.einsum(\"bhid, bhjd -> bhij\", q, k) * self.scales\n",
        "\n",
        "    if mask is not None:\n",
        "      mask = tf.pad(mask.flatten(1), (1, 0), constant_values=True)\n",
        "      assert mask.shape[-1] == dots.shape[-1], \"Mask has incorrect dimensions\"\n",
        "      mask = mask[:, None, :] * mask[:, :, None]\n",
        "      dots[~mask] = tf.fill(dots[~mask], float('-inf'))\n",
        "      del mask\n",
        "\n",
        "    attn = tf.nn.softmax(dots, axis=-1)\n",
        "    out = tf.einsum(\"bhij,bhjd->bhid\", attn, v)\n",
        "    out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "    out = self.to_out(out, training=True)\n",
        "    return out"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Wof6loPBHcq"
      },
      "source": [
        "class Transformer(layers.Layer):\n",
        "  def __init__(self, dim, depth, heads, mlp_dim, dropout): \n",
        "    super().__init__()\n",
        "    self.layers = []\n",
        "\n",
        "    for _ in range(depth): \n",
        "      self.layers.append([Residual(PreNorm(dim, Attention(dim, heads=heads, dropout=dropout))), \n",
        "                          Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)))])\n",
        "      \n",
        "  def call(self, x, mask=None):\n",
        "    for attn, ff in self.layers:\n",
        "      x = attn(x, mask=mask)\n",
        "      x = ff(x)\n",
        "    return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qoirJ7DC6k_"
      },
      "source": [
        "MIN_NUM_PATCHES=16\n",
        "class ViT(tf.keras.Model):\n",
        "  def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dropout=0., emb_dropout=0.):\n",
        "    super().__init__()\n",
        "    assert image_size % patch_size == 0, \"Image size must be divisible for the patch size\"\n",
        "    num_patches = (image_size // patch_size) ** 2\n",
        "    patch_dim = channels * patch_size**2\n",
        "    assert num_patches >= MIN_NUM_PATCHES,  f'your number of patches ({num_patches}) is way too small for attention to be effective. try decreasing your patch size'\n",
        "\n",
        "    self.patch_size = patch_size \n",
        "\n",
        "    self.pos_embedding = tf.Variable(tf.random.normal([1, num_patches+1, dim]))\n",
        "    self.patch_to_embedding = layers.Dense(dim, input_shape=(patch_dim,), use_bias=False)\n",
        "    self.cls_token = tf.Variable(tf.random.normal([1, 1, dim]))\n",
        "    self.dropout = layers.Dropout(emb_dropout)\n",
        "\n",
        "    self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)\n",
        "\n",
        "\n",
        "    self.mlp_layer_norm = layers.LayerNormalization(axis=-1)\n",
        "    self.mlp_dense1 = layers.Dense(mlp_dim)\n",
        "    self.mlp_gelu = tfa.layers.GELU() \n",
        "    self.mlp_dropout = layers.Dropout(dropout)\n",
        "    self.mlp_dense2 = layers.Dense(num_classes)\n",
        "\n",
        "    # self.mlp_head = tf.keras.Sequential(layers=[\n",
        "    #                                             layers.LayerNormalization(axis=-1), \n",
        "    #                                             layers.Dense(mlp_dim),\n",
        "    #                                             tfa.layers.GELU(), \n",
        "    #                                             layers.Dropout(dropout), \n",
        "    #                                             layers.Dense(num_classes)\n",
        "    # ])\n",
        "\n",
        "  def mlp_head(self, x, training=True):\n",
        "    out = self.mlp_layer_norm(x)\n",
        "    out = self.mlp_dense1(out)\n",
        "    out = self.mlp_gelu(out)\n",
        "    out = self.mlp_dropout(out, training=training)\n",
        "    out = self.mlp_dense2(out)\n",
        "    return out\n",
        "\n",
        "  def call(self, img, mask=None, training=True):\n",
        "    p = self.patch_size\n",
        "    x = rearrange(img, \"b (h p1) (w p2) c -> b (h w) (p1 p2 c)\", p1=p, p2=p)\n",
        "    x = self.patch_to_embedding(x)\n",
        "    b, n, _ = x.shape\n",
        "    #pdb.set_trace()\n",
        "    cls_token = tf.repeat(self.cls_token, [b], axis=0)\n",
        "    x = tf.concat([cls_token, x], axis=1)\n",
        "    x += self.pos_embedding[:, :(n+1)]\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.transformer(x, mask)\n",
        "    x = tf.identity(x[:, 0])\n",
        "\n",
        "    return self.mlp_head(x, training=training)\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17oXsv77yQlx"
      },
      "source": [
        "train_aug_layers = tf.keras.Sequential(layers=[layers.ZeroPadding2D(padding=(4, 4)), \n",
        "                                             tf.keras.layers.experimental.preprocessing.RandomCrop(32, 32), \n",
        "                                             tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "                                             tf.keras.layers.experimental.preprocessing.Normalization(axis=-1, mean=(0.4914, 0.4822, 0.4465), variance=(0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "test_aug_layers = tf.keras.layers.experimental.preprocessing.Normalization(axis=-1, mean=(0.4914, 0.4822, 0.4465), variance=(0.2023, 0.1994, 0.2010))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDyxH1QmY_nO"
      },
      "source": [
        "def train_aug_func(ds_sample):\n",
        "  image = ds_sample['image']\n",
        "  label = ds_sample['label']\n",
        "  aug_image = train_aug_layers(image)\n",
        "  one_hot_label = tf.one_hot(label, depth=10)\n",
        "  return aug_image, one_hot_label"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvo9RiqTZKY8"
      },
      "source": [
        "def test_aug_func(ds_sample): \n",
        "  image = ds_sample[\"image\"]\n",
        "  label = ds_sample[\"label\"]\n",
        "  aug_image = test_aug_layers(image)\n",
        "  one_hot_label = tf.one_hot(label, depth=10)\n",
        "  return aug_image, one_hot_label"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br1zIfg10tou"
      },
      "source": [
        "cifar10_train = tfds.load(\"cifar10\", split=\"train\")\n",
        "cifar10_test = tfds.load(\"cifar10\", split=\"test\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "megZMz-6zFEZ"
      },
      "source": [
        "def get_train_dataset(batch_size):\n",
        "  ds = cifar10_train.shuffle(10000)\n",
        "  ds = ds.batch(batch_size, drop_remainder=True)\n",
        "  ds = ds.map(train_aug_func)\n",
        "  ds = ds.prefetch(10000)\n",
        "  return ds"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3F5EtLdbq1I"
      },
      "source": [
        "def get_test_dataset(batch_size):\n",
        "  ds = cifar10_test.shuffle(10000)\n",
        "  ds = ds.batch(batch_size, drop_remainder=True)\n",
        "  ds = ds.map(test_aug_func)\n",
        "  ds = ds.prefetch(10000)\n",
        "  return ds"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCaaaOjkJCit"
      },
      "source": [
        "class WarmUpScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=2000):\n",
        "    super(WarmUpScheduler, self).__init__()\n",
        "    self.d_model = tf.cast(d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps \n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZDSQ3-Ib1xz"
      },
      "source": [
        "import os\n",
        "from tqdm import notebook\n",
        "import albumentations\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ltyqoEyeXPe"
      },
      "source": [
        "config = {\"lr\": 1e-4, \n",
        "          \"batch_size\": 64, \n",
        "          \"n_epochs\": 100, \n",
        "          \"patch\": 2}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKQ680b8dvXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "74a526ae-bfd3-46d4-8e86-f1a2dd22e063"
      },
      "source": [
        "wandb.init(project=\"ViT-cifar10-tf\", entity=\"caonv\", config=config)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcaonv\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">flowing-cosmos-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/caonv/ViT-cifar10-tf\" target=\"_blank\">https://wandb.ai/caonv/ViT-cifar10-tf</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/caonv/ViT-cifar10-tf/runs/23patj9j\" target=\"_blank\">https://wandb.ai/caonv/ViT-cifar10-tf/runs/23patj9j</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210513_082201-23patj9j</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f6f9830dcd0>"
            ],
            "text/html": [
              "<h1>Run(23patj9j)</h1><iframe src=\"https://wandb.ai/caonv/ViT-cifar10-tf/runs/23patj9j\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvXIZGS-ilm4"
      },
      "source": [
        "best_acc = 0\n",
        "start_epoch = 0 "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl2nBwari_do"
      },
      "source": [
        "#config = wandb.config"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHKNZPTmmPN5"
      },
      "source": [
        "net = ViT(image_size = 32,\n",
        "    patch_size = 4, \n",
        "    num_classes = 10,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 512,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWOUG54jnboT"
      },
      "source": [
        "train_ds = get_train_dataset(config[\"batch_size\"])\n",
        "test_ds = get_test_dataset(config[\"batch_size\"])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp-lcg0-xhEV"
      },
      "source": [
        "lr_reduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1,\n",
        "                                                    patience=3, min_delta=0.01, min_lr=1e-8, cooldown=0)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSaCHKApmQmw"
      },
      "source": [
        "net.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=config[\"lr\"]),\n",
        "                  metrics = [\"accuracy\"])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJi8MdAVM_9y"
      },
      "source": [
        "callbacks = [lr_reduce_on_plateau, WandbCallback()]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtntKXujt5MR",
        "outputId": "3ae231b8-13df-43e2-dadc-f1d4ff42a95f"
      },
      "source": [
        "net.fit(train_ds, epochs=100, validation_data=test_ds, callbacks=[WandbCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:390: UserWarning: Default value of `approximate` is changed from `True` to `False`\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  6/781 [..............................] - ETA: 1:29 - loss: 2.5384 - accuracy: 0.0939WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0176s vs `on_train_batch_end` time: 0.0959s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0176s vs `on_train_batch_end` time: 0.0959s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 103s 122ms/step - loss: 2.0919 - accuracy: 0.2097 - val_loss: 1.6592 - val_accuracy: 0.3713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/100\n",
            "781/781 [==============================] - 93s 120ms/step - loss: 1.6690 - accuracy: 0.3756 - val_loss: 1.5103 - val_accuracy: 0.4426\n",
            "Epoch 3/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.4959 - accuracy: 0.4492 - val_loss: 1.4022 - val_accuracy: 0.4838\n",
            "Epoch 4/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.4079 - accuracy: 0.4890 - val_loss: 1.3361 - val_accuracy: 0.5139\n",
            "Epoch 5/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.3507 - accuracy: 0.5070 - val_loss: 1.3111 - val_accuracy: 0.5191\n",
            "Epoch 6/100\n",
            "781/781 [==============================] - 93s 120ms/step - loss: 1.3049 - accuracy: 0.5332 - val_loss: 1.2677 - val_accuracy: 0.5366\n",
            "Epoch 7/100\n",
            "781/781 [==============================] - 93s 120ms/step - loss: 1.2773 - accuracy: 0.5416 - val_loss: 1.2623 - val_accuracy: 0.5500\n",
            "Epoch 8/100\n",
            "781/781 [==============================] - 93s 120ms/step - loss: 1.2498 - accuracy: 0.5508 - val_loss: 1.2380 - val_accuracy: 0.5562\n",
            "Epoch 9/100\n",
            "781/781 [==============================] - 93s 120ms/step - loss: 1.2201 - accuracy: 0.5578 - val_loss: 1.2025 - val_accuracy: 0.5702\n",
            "Epoch 10/100\n",
            "781/781 [==============================] - 93s 120ms/step - loss: 1.2031 - accuracy: 0.5680 - val_loss: 1.2359 - val_accuracy: 0.5598\n",
            "Epoch 11/100\n",
            "781/781 [==============================] - 93s 120ms/step - loss: 1.1881 - accuracy: 0.5741 - val_loss: 1.1981 - val_accuracy: 0.5696\n",
            "Epoch 12/100\n",
            "781/781 [==============================] - 93s 120ms/step - loss: 1.1629 - accuracy: 0.5831 - val_loss: 1.1778 - val_accuracy: 0.5749\n",
            "Epoch 13/100\n",
            "781/781 [==============================] - 93s 120ms/step - loss: 1.1459 - accuracy: 0.5864 - val_loss: 1.1494 - val_accuracy: 0.5939\n",
            "Epoch 14/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.1344 - accuracy: 0.5928 - val_loss: 1.1597 - val_accuracy: 0.5896\n",
            "Epoch 15/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.1168 - accuracy: 0.5989 - val_loss: 1.1542 - val_accuracy: 0.5894\n",
            "Epoch 16/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.1154 - accuracy: 0.5990 - val_loss: 1.1429 - val_accuracy: 0.5894\n",
            "Epoch 17/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.0910 - accuracy: 0.6098 - val_loss: 1.1123 - val_accuracy: 0.6116\n",
            "Epoch 18/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.0786 - accuracy: 0.6131 - val_loss: 1.1012 - val_accuracy: 0.6102\n",
            "Epoch 19/100\n",
            "781/781 [==============================] - 93s 120ms/step - loss: 1.0793 - accuracy: 0.6116 - val_loss: 1.0923 - val_accuracy: 0.6056\n",
            "Epoch 20/100\n",
            "781/781 [==============================] - 93s 120ms/step - loss: 1.0579 - accuracy: 0.6210 - val_loss: 1.0880 - val_accuracy: 0.6103\n",
            "Epoch 21/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.0465 - accuracy: 0.6239 - val_loss: 1.0958 - val_accuracy: 0.6162\n",
            "Epoch 22/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.0417 - accuracy: 0.6278 - val_loss: 1.1160 - val_accuracy: 0.6075\n",
            "Epoch 23/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.0393 - accuracy: 0.6291 - val_loss: 1.0752 - val_accuracy: 0.6177\n",
            "Epoch 24/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.0190 - accuracy: 0.6357 - val_loss: 1.0770 - val_accuracy: 0.6191\n",
            "Epoch 25/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.0164 - accuracy: 0.6348 - val_loss: 1.0564 - val_accuracy: 0.6224\n",
            "Epoch 26/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 1.0094 - accuracy: 0.6362 - val_loss: 1.0492 - val_accuracy: 0.6231\n",
            "Epoch 27/100\n",
            "781/781 [==============================] - 94s 120ms/step - loss: 0.9946 - accuracy: 0.6463 - val_loss: 1.0363 - val_accuracy: 0.6294\n",
            "Epoch 28/100\n",
            "181/781 [=====>........................] - ETA: 1:07 - loss: 1.0200 - accuracy: 0.6400"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRXuPGNBpEQW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}